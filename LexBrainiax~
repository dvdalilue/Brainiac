#!/usr/bin/env ruby

###########################
#Enumeracion de los Tokens#
###########################

#Creacion de un hash de las palabras claves
rw = Hash::new
reserved_words = %w(declare execute done read write while do if else end from at tape to true false integer boolean)

#Se van agregando iterativamente al hash con su regex
reserved_words.each do |s|
  rw[s.capitalize] = /\A#{s}\b/
end

#Declaracion manual de los tokens con sus expresiones regulares

tk = {

  'Cinta'           =>  /\A{[+-<>.,]*}/               ,      
  'ConstructorTape' =>  /\A\[[a-zA-Z_][0-9a-zA-Z_]*\]/,
  'Coma'            =>  /\A\,/                        ,      
  'Punto'           =>  /\A\./                        ,       
  'PuntoYComa'      =>  /\A\;/                        ,       
  'ParAbre'         =>  /\A\(/                        ,       
  'ParCierra'       =>  /\A\)/                        ,       
  'CorcheteAbre'    =>  /\A\[/                        ,       
  'CorcheteCierra'  =>  /\A\]/                        ,       
  'LlaveAbre'       =>  /\A\{/                        ,       
  'LlaveCierra'     =>  /\A\}/                        ,       
  'Type'            =>  /\A\:\:/                      ,       
  'Menos'           =>  /\A\-/                        ,      
  'Mas'             =>  /\A\+/                        ,      
  'Mult'            =>  /\A\*/                        ,       
  'Div'             =>  /\A\/(?![\\=])/               ,       
  'Mod'             =>  /\A\%/                        ,       
  'Conjuncion'      =>  /\A\/\\/                      ,       
  'Disyuncion'      =>  /\A\\\//                      ,       
  'Negacion'        =>  /\A\~/                        ,       
  'Menor'           =>  /\A\<(?!=)/                   ,       
  'MenorIgual'      =>  /\A\<=/                       ,       
  'Mayor'           =>  /\A\>(?!=)/                   ,       
  'MayorIgual'      =>  /\A\>=/                       ,       
  'Igual'           =>  /\A\=/                        ,       
  'Desigual'        =>  /\A\/=/                       ,       
  'Concat'          =>  /\A\&/                        ,       
  'Inspeccion'      =>  /\A\#/                        ,      
  'Asignacion'      =>  /\A\:=/                       ,     
  'Ident'           =>  /\A[a-zA-Z_][0-9a-zA-Z_]*/    ,
  'Error'           =>  /\A\W/                        ,   
  'Num'             =>  /\A\d*/                       ,

}

#Se unen ambas hash primera las palabras claves, evitar que sean confundidas por "Ident"

$tokens = rw.merge(tk)

###########################
#Declaracion de las clases#
###########################

#Clase general para guardar un texto, linea y columna

class PhraseS

  attr_accessor :text, :line, :column # Puede ser leido y escribir en ellos

  def initialize(text, line, column)
    @text = text
    @line = line
    @column = column
  end
end

# Clase de error lexicografico con todo de la PhraseS

class LexicographError < PhraseS

  def to_s #Salida especial del enunciado
    "Error: Caracter inesperado \"#{@text}\" en la fila #{@line}, columna #{@column}"
  end
end

#Clase para Token con todo de PhraseS

class Token < PhraseS

  class << self
    attr_accessor :regex #Esta seccion es para que la clase "redefinir la clase" 
  end                   #Se conoce como sigleton

  def to_s#Salida especificada
    "#{self.class.name}#{if self.class.name.eql?("TkIdent") then "(\"#{@text}\")" end}#{if self.class.name.eql?("TkNum") then "(#{@text})" end} "
  end
end

#Clase fundamental que realiza el recorrido de la entrada
#Diferencia de los tokens y errores
#Para luego ser impresos en pantalla

class Lexer

  #Inicializacion con la entrada como Stirng
  def initialize(input)
    @tokens = [] # Arreglo de tokens
    @errors = [] #Arreglo de errores
    @input = input
    @line = 1
    @column = 1
    @comment = 0
  end

  #Ignora una cantidad de columnas determinada por "legnth" 
  def lex_ignore_c(length)
    @column  += length
  end

  #Ignora la palabra que se analizo o espacios en blanco
  def lex_ignore(length)
    
    return if length.eql?0 #Si no hay nada regresa

    word = @input[0..length] # Se crea un aux de lo que se quiere ignorar
    lineas = (word + ' ').lines.to_a.length.pred #Se saca el numero de lineas, 
                                  #convirtiendo en arreglo de las palabras separadas \n y midiendolo
    @line += lineas
    @input = @input[length..@input.length] # Se omite la solicitado

    if lineas.eql?0 then
      lex_ignore_c(length) #Se suma las columnas omitidas a las que habia
    else
      @column = 1 #Sino se colocan en 1 por salto de linea
    end
  end

  #Convierte en token la frase
  def tokenize(nphrase)

    nct = LexicographError #Se define una var con el nombre de la clase
    ntt = nphrase         #Consigo el texto para esa clase

    if nphrase =~ /\A\$-.*/
      @comment = 1 #Verifica si es inicio de comentario
    else
      $tokens.each { |key, value|
        if nphrase =~ value       #Sino busca con quien hacer match en las regexp
          nct = "Tk" + "#{key}"
          ntt = $&
          break
        end
      }

      if nct == "TkError"
        nct = LexicographError  #Si hubo match con error se le coloca esta clase
      else      
        nct = Object.const_get(nct)  #Debe buscar la clase en todos los objetos correspondiente al string que se localizo
      end

      newtk = nct.new(ntt,@line,@column) # se crea la nueva instancia de la clase
      
      if newtk.is_a? LexicographError 
        @errors << newtk  #Si es un error se guarda en este arreglo
      else
        @tokens << newtk #Sino en este, por ser token
        if ntt.length < nphrase.length
          lex_ignore_c(ntt.length)  #Caso especial en el que los tokens estan pegados
          tokenize(nphrase[ntt.length..nphrase.length]) #Se ignora y se tokeniza lo que falta
        end
      end
    end
  end

  #Va agarrando la palabras separadas por espacio o por saltos de linea
  def lex_catch

    return false if @input.eql?(nil) #retorna si la entrada se acabo, con false

    @input.match(/\A(\s|\n)*/) #ignora los espacios y saltos de linea
    lex_ignore($&.length)
    @input.match(/\A[\w\p{punct}]*\s/) #Busca la proxima palabra

    if $&.eql?nil
      lex_ignore(1) #Si el match es nil se sigue al siguiente caracter
    else
      mlength = $&.length 
      if @comment == 0
        tokenize(@input[0..($&.length-2)]) #Se tokeniza si no estamos en comentario
      elsif $& =~ /\A.*-\$/
        @comment = 0 #Si se consigue cierre de comentario, cambiamos el flag
      end
      lex_ignore(mlength) #Ignora la palabra analizada
    end
  end

  def to_s #Dependiendo de si hubo errores o no, cambia la salida
    (if @errors.empty? then @tokens else @errors end).map { |tk| puts tk.inspect }
  end
end

#######################################
#Declaracion de clases para cada token#
#######################################

#Creacion de clases de manera dinamica para cada token

$tokens.each do |id,regex|

  newclass = Class::new(Token) do #Creacion dinamica "Magic"

    @regex = regex #Se le asigna la regexp respectiva a la clase
    
    def initialize(text, line, column)
      @text = text
      @line = line
      @column = column
    end
  end

  Object::const_set("Tk#{id}", newclass) #Se renombra la clase como Tk<nombre>

end

###############################
#Definicion del Main del Lexer#
###############################

def main

  input = File::read(ARGV[0]) #Lectura de entrada en un string
  lexer = Lexer::new (input) #Creacion de un lexer
  goon = true

  while (goon) do
    goon = lexer.lex_catch #Se van racogiendo la palabras hasta que diga false
  end
  lexer.to_s #Salida por pantalla como se especifico
end

main
